import os
import re
import time
from threading import Lock
from typing import Any, Dict, List

import torch
from dotenv import load_dotenv
from transformers import AutoModelForCausalLM, AutoTokenizer
from utils.logger import logger

load_dotenv(override=True)


# ðŸ’¡ ë¹„ì†ì–´ íŒ¨í„´ í•„í„° (ìš°íšŒ í‘œí˜„ í¬í•¨)
BADWORD_PATTERNS = [
    r"\bã……ã…‚\b",
    r"\bã…‚ã……\b",
    r"\bã…—\b",
    r"\bfuck\b",
    r"\bbitch\b",
    r"\basshole\b",
]


def matches_badword(text: str) -> bool:
    for pattern in BADWORD_PATTERNS:
        if re.search(pattern, text, re.IGNORECASE):
            return True
    return False


class HyperCLOVAXLoader:
    def __init__(
        self, model_name: str = "naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B"
    ):
        self.model_name = model_name
        torch.set_num_threads(os.cpu_count())
        torch.set_num_interop_threads(2)
        self.device = "cpu"
        logger.info(f"Using device: {self.device}")

        start_time = time.time()
        logger.info(f"Loading tokenizer from {model_name}...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name, token=os.getenv("HUGGINGFACE_TOKEN")
        )

        logger.info(f"Loading model from {model_name}...")
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float32,
            token=os.getenv("HUGGINGFACE_TOKEN"),
        ).to(self.device)

        self.generation_config = {
            "temperature": 0.1,
            "top_p": 0.9,
            "max_new_tokens": 16,
            "do_sample": False,
        }

        load_time = time.time() - start_time
        logger.info(f"Model loading completed in {load_time:.2f} seconds")

    def classify(self, text: str) -> Dict[str, str]:
        messages = [
            {
                "role": "system",
                "content": (
                    "ë‹¹ì‹ ì€ ì‚¬ìš©ìžì˜ ë©”ì‹œì§€ê°€ ìœ í•´í•œì§€ íŒë‹¨í•˜ëŠ” ì¸ê³µì§€ëŠ¥ìž…ë‹ˆë‹¤. "
                    "ìœ í•´í•œ ë©”ì‹œì§€ì—ëŠ” ìš•ì„¤, ë¹„ë°©, ì°¨ë³„, ì„±ì  ë‚´ìš©, í­ë ¥ì ì¸ í‘œí˜„ì´ í¬í•¨ë  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. "
                    "ì˜ˆ: ì‹œ11ë°œ, ã……ã…‚, ã…‚ã……, ã…— ë“±ì˜ í‘œí˜„ë„ ìœ í•´ë¡œ ê°„ì£¼í•´ì•¼ í•©ë‹ˆë‹¤. "
                    "ìœ í•´í•œ ê²½ìš°ì—ëŠ” 'ìœ í•´'ë¼ê³ ë§Œ ë‹µí•˜ê³ , ì•„ë‹ˆë¼ë©´ 'ì•ˆì „'ì´ë¼ê³ ë§Œ ë‹µí•˜ì„¸ìš”."
                ),
            },
            {
                "role": "user",
                "content": f'ë©”ì‹œì§€: "{text}"\n\nì´ ë©”ì‹œì§€ëŠ” ìœ í•´í•œê°€ìš”?',
            },
        ]
        response = self.get_response(messages)
        result_text = response.get("content", "").lower()

        is_model_toxic = any(
            keyword in result_text for keyword in ["ìœ í•´", "ì œìž¬", "ë¶€ì ì ˆ"]
        )
        is_pattern_toxic = matches_badword(text)

        label = "toxic" if (is_model_toxic or is_pattern_toxic) else "safe"

        return {"label": label, "raw": result_text}

    def get_response(self, messages: List[Dict[str, str]]) -> Dict[str, Any]:
        start_time = time.time()
        try:
            prompt = self.tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            inputs = self.tokenizer(prompt, return_tensors="pt")
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            with torch.no_grad():
                outputs = self.model.generate(**inputs, **self.generation_config)
            generated_text = self.tokenizer.decode(
                outputs[0][inputs["input_ids"].shape[1] :], skip_special_tokens=True
            )
            end_time = time.time()
            logger.info(f"Inference completed in {end_time - start_time:.2f} seconds")
            return {
                "status_code": 200,
                "content": generated_text.strip(),
                "inference_time": end_time - start_time,
            }
        except Exception as e:
            logger.error(f"Error generating response: {str(e)}")
            return {"status_code": 500, "error": str(e)}


class ModelSingleton:
    _instance = None
    _lock = Lock()

    @classmethod
    def get_instance(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    model_name = os.getenv(
                        "MODEL_NAME",
                        "naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B",
                    )
                    logger.info(
                        f"Initializing HyperCLOVAXLoader with model: {model_name}"
                    )
                    cls._instance = HyperCLOVAXLoader(model_name)
        return cls._instance
