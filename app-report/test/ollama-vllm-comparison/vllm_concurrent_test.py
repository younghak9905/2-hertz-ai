import asyncio
import statistics
import subprocess
import time

import GPUtil
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent


def get_vram_usage():
    gpus = GPUtil.getGPUs()
    if gpus:
        return gpus[0].memoryUsed  # MB ë‹¨ìœ„
    return 0


async def test_vllm():
    # ëª¨ë“  ëª¨ë¸ ì–¸ë¡œë“œ
    # subprocess.run(["ollama", "stop", "qwen2.5:7b"], capture_output=True)
    time.sleep(2)  # ì–¸ë¡œë“œ ëŒ€ê¸°

    # ëª¨ë¸ ì´ˆê¸°í™”
    inference_server_url = "http://localhost:8000/v1"
    vllm_llm = ChatOpenAI(
        model="Qwen/Qwen2.5-7B-Instruct-AWQ",
        openai_api_key="EMPTY",
        openai_api_base=inference_server_url,
    )

    # ê°„ë‹¨í•œ ì—ì´ì „íŠ¸ ìƒì„±(ë„êµ¬ ì—†ìŒ)
    agent = create_react_agent(vllm_llm, [])

    # # í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€1
    # messages = [
    #     {"role": "user", "content": "55x55ëŠ”?"},
    # ]
    # í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€2
    messages = [
        {
            "role": "system",
            "content": "ë‹¹ì‹ ì€ ì „ë¬¸ ê¸°ìì…ë‹ˆë‹¤. ì •í™•í•˜ê³  ê°ê´€ì ì¸ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”. ë‹µë³€ì€ json í˜•ì‹ìœ¼ë¡œ ì•„ë˜ì²˜ëŸ¼ êµ¬ì„±í•˜ì„¸ìš”:\n\n"
            '{ "title": "ë‰´ìŠ¤ ì œëª©", "content": "ë‰´ìŠ¤ ë‚´ìš©" }',
        },
        {
            "role": "user",
            "content": "AI ê¸°ìˆ  ë°œì „ì— ëŒ€í•œ ì§§ì€ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.",
        },
    ]

    # ëª¨ë¸ ì‹¤í–‰ ì „ VRAM
    vram_before = get_vram_usage()

    # ëª¨ë¸ ì‹¤í–‰ ë° ì‹œê°„ ì¸¡ì •
    start_time = time.time()
    model_response = await agent.ainvoke({"messages": messages})
    end_time = time.time()

    print(f"\nëª¨ë¸ ì‘ë‹µ:\n{model_response}")

    # ëª¨ë¸ ì‹¤í–‰ ì „ VRAM
    vram_after = get_vram_usage()

    # ëª¨ë¸ ì‘ë‹µ í™•ì¸
    ai_message = model_response["messages"][-1]
    print(f"\nëª¨ë¸ ì‘ë‹µ:\n{ai_message}")

    # ì‘ë‹µì‹œê°„ í™•ì¸
    response_time = end_time - start_time
    print(f"\nì‘ë‹µì‹œê°„: {response_time:.3f}ì´ˆ")

    # í† í° í™•ì¸
    token_usage = model_response["messages"][-1].usage_metadata
    token_per_second = token_usage["output_tokens"] / response_time
    print(f"\ní† í° ìˆ˜:\n{token_usage}")
    print(f"ì´ˆë‹¹ ì²˜ë¦¬ í† í° ìˆ˜:\n{token_per_second}")

    # VRAM ì‚¬ìš©ëŸ‰ í™•ì¸(vLLM ì„œë²„ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ê¸°ì— ì¸¡ì • ë¶ˆê°€)
    vram_used = vram_after - vram_before
    print(f"\nVRAM ì‚¬ìš©ëŸ‰:\n {vram_used}MB")


async def concurrent_request(llm, agent, request_id):
    """ë‹¨ì¼ ë™ì‹œ ìš”ì²­ ì²˜ë¦¬"""
    messages = [
        {
            "role": "user",
            "content": f"ìš”ì²­ {request_id}: {request_id * 10 + 5}ì˜ ì œê³±ê³¼ ì œê³±ê·¼ì„ êµ¬í•´ì£¼ì„¸ìš”.",
        }
    ]

    try:
        start_time = time.time()
        response = await agent.ainvoke({"messages": messages})
        end_time = time.time()

        ai_message = response["messages"][-1]
        response_time = end_time - start_time
        token_usage = ai_message.usage_metadata

        print(f"\nëª¨ë¸ ì‘ë‹µ:\n{ai_message}")

        return {
            "request_id": request_id,
            "success": True,
            "response_time": response_time,
            "tokens_per_second": token_usage["output_tokens"] / response_time,
            "output_tokens": token_usage["output_tokens"],
        }
    except Exception as e:
        return {"request_id": request_id, "success": False, "error": str(e)}


async def test_concurrent_ollama(num_requests=3):
    """ë™ì‹œ ìš”ì²­ í…ŒìŠ¤íŠ¸ - ê¸°ì¡´ ì½”ë“œì— ì¶”ê°€í•˜ê¸° ì¢‹ì€ ê°„ë‹¨í•œ ë²„ì „"""
    print(f"\n{'='*50}")
    print(f"ğŸ§ª ë™ì‹œ ìš”ì²­ í…ŒìŠ¤íŠ¸ ({num_requests}ê°œ)")
    print(f"{'='*50}")

    # ëª¨ë“  ëª¨ë¸ ì–¸ë¡œë“œ
    subprocess.run(["ollama", "stop", "qwen2.5:7b"], capture_output=True)
    time.sleep(2)

    # ë©”ëª¨ë¦¬ ì¸¡ì • ì‹œì‘
    vram_before = get_vram_usage()

    # ëª¨ë¸ ì´ˆê¸°í™”
    inference_server_url = "http://localhost:8000/v1"
    vllm_llm = ChatOpenAI(
        model="Qwen/Qwen2.5-7B-Instruct-AWQ",
        openai_api_key="EMPTY",
        openai_api_base=inference_server_url,
    )
    agent = create_react_agent(vllm_llm, [])

    # ë™ì‹œ ìš”ì²­ ì‹¤í–‰
    print(f"â±ï¸  {num_requests}ê°œ ë™ì‹œ ìš”ì²­ ì‹œì‘...")
    overall_start = time.time()

    tasks = [concurrent_request(vllm_llm, agent, i + 1) for i in range(num_requests)]
    results = await asyncio.gather(*tasks)

    overall_end = time.time()
    overall_time = overall_end - overall_start

    # ë©”ëª¨ë¦¬ ì¸¡ì • ì¢…ë£Œ
    vram_after = get_vram_usage()

    # ê²°ê³¼ ë¶„ì„
    successful = [r for r in results if r.get("success", False)]
    failed = [r for r in results if not r.get("success", False)]

    print(f"\nğŸ“Š ê²°ê³¼:")
    print(f"   âœ… ì„±ê³µ: {len(successful)}/{num_requests}")
    print(f"   âŒ ì‹¤íŒ¨: {len(failed)}")
    print(f"   â±ï¸  ì „ì²´ ì‹œê°„: {overall_time:.2f}ì´ˆ")

    if successful:
        response_times = [r["response_time"] for r in successful]
        tokens_per_sec = [r["tokens_per_second"] for r in successful]

        print(f"   ğŸ“ˆ í‰ê·  ì‘ë‹µì‹œê°„: {statistics.mean(response_times):.2f}ì´ˆ")
        print(f"   ğŸš€ í‰ê·  í† í°/ì´ˆ: {statistics.mean(tokens_per_sec):.1f}")
        print(f"   ğŸ“Š ì´ í† í°: {sum(r['output_tokens'] for r in successful)}")

    print(f"   ğŸ’¾ VRAM ì‚¬ìš©: {vram_after - vram_before}MB")

    # ê°œë³„ ê²°ê³¼
    print(f"\nğŸ“‹ ê°œë³„ ê²°ê³¼:")
    for result in successful:
        print(f"   ìš”ì²­ {result['request_id']}: {result['response_time']:.2f}ì´ˆ")

    for result in failed:
        print(f"   âŒ ìš”ì²­ {result['request_id']}: {result['error']}")


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("Ollama í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤!")
    print("\n1. ë‹¨ì¼ ìš”ì²­ í…ŒìŠ¤íŠ¸")
    print("2. ë™ì‹œ ìš”ì²­ í…ŒìŠ¤íŠ¸ (3ê°œ)")
    print("3. ë™ì‹œ ìš”ì²­ í…ŒìŠ¤íŠ¸ (5ê°œ)")
    print("4. ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰")

    choice = input("\nì„ íƒí•˜ì„¸ìš” (1-4): ").strip()

    if choice == "1":
        await test_vllm()
    elif choice == "2":
        await test_concurrent_ollama(3)
    elif choice == "3":
        await test_concurrent_ollama(5)
    elif choice == "4":
        print("\nğŸ”„ ëª¨ë“  í…ŒìŠ¤íŠ¸ë¥¼ ìˆœì°¨ ì‹¤í–‰í•©ë‹ˆë‹¤...")

        # ë‹¨ì¼ ìš”ì²­ í…ŒìŠ¤íŠ¸
        await test_vllm()

        # 3ì´ˆ ëŒ€ê¸°
        print("\nâ³ 3ì´ˆ í›„ ë™ì‹œ ìš”ì²­ í…ŒìŠ¤íŠ¸...")
        await asyncio.sleep(3)

        # 3ê°œ ë™ì‹œ ìš”ì²­
        await test_concurrent_ollama(3)

        # 3ì´ˆ ëŒ€ê¸°
        print("\nâ³ 3ì´ˆ í›„ 5ê°œ ë™ì‹œ ìš”ì²­ í…ŒìŠ¤íŠ¸...")
        await asyncio.sleep(3)

        # 5ê°œ ë™ì‹œ ìš”ì²­
        await test_concurrent_ollama(5)

        print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    else:
        print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. ê¸°ë³¸ìœ¼ë¡œ ë‹¨ì¼ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        await test_vllm()


if __name__ == "__main__":
    asyncio.run(main())
